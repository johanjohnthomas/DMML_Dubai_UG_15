{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from glob import glob\n",
    "from PIL import ImageFile\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Suppress PIL warnings\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True  # Handle truncated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "ROOT_DIR = \"Datasets/corrected_wildfires_dataset\"\n",
    "IMAGE_SIZE = (224, 224)\n",
    "NUM_CLASSES = 2\n",
    "EPOCHS = 10\n",
    "NUM_FOLDS = 10\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before balancing: {0: 469, 1: 990}\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset and labels\n",
    "image_paths = glob(f\"{ROOT_DIR}/*/*.jpg\")\n",
    "class_labels = {\"fire\": 0, \"nofire\": 1}\n",
    "\n",
    "# Filter image paths and extract labels\n",
    "filtered_image_paths = []\n",
    "labels = []\n",
    "for p in image_paths:\n",
    "    label_name = os.path.basename(os.path.dirname(p))\n",
    "    if label_name in class_labels:\n",
    "        filtered_image_paths.append(p)\n",
    "        labels.append(class_labels[label_name])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "image_paths = np.array(filtered_image_paths)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Check class distribution before balancing\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "class_counts = dict(zip(unique, counts))\n",
    "print(\"Class distribution before balancing:\", class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after balancing: {0: 469, 1: 469}\n"
     ]
    }
   ],
   "source": [
    "# Balance the classes by undersampling the majority class\n",
    "min_count = min(class_counts.values())\n",
    "\n",
    "balanced_image_paths = []\n",
    "balanced_labels = []\n",
    "\n",
    "for class_label in np.unique(labels):\n",
    "    class_indices = np.where(labels == class_label)[0]\n",
    "    sampled_indices = np.random.choice(class_indices, min_count, replace=False)\n",
    "    balanced_image_paths.extend(image_paths[sampled_indices])\n",
    "    balanced_labels.extend([class_label] * min_count)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "balanced_image_paths = np.array(balanced_image_paths)\n",
    "balanced_labels = np.array(balanced_labels)\n",
    "\n",
    "# Shuffle the balanced dataset\n",
    "indices = np.arange(len(balanced_image_paths))\n",
    "np.random.shuffle(indices)\n",
    "balanced_image_paths = balanced_image_paths[indices]\n",
    "balanced_labels = balanced_labels[indices]\n",
    "\n",
    "# Update image_paths and labels to the balanced dataset\n",
    "image_paths = balanced_image_paths\n",
    "labels = balanced_labels\n",
    "\n",
    "# Check class distribution after balancing\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "class_counts = dict(zip(unique, counts))\n",
    "print(\"Class distribution after balancing:\", class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    image_paths, labels, test_size=0.2, stratify=labels, random_state=seed\n",
    ")\n",
    "\n",
    "# Create reverse mapping from label indices to class names\n",
    "rev_class_labels = {v: k for k, v in class_labels.items()}\n",
    "\n",
    "# Create DataFrames for generators with string labels\n",
    "train_df = pd.DataFrame({\n",
    "    'filename': train_paths,\n",
    "    'class': [rev_class_labels[label] for label in train_labels]\n",
    "})\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    'filename': val_paths,\n",
    "    'class': [rev_class_labels[label] for label in val_labels]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    brightness_range=[0.9, 1.1]\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define diverse architectures\n",
    "architectures = [\n",
    "     {\n",
    "        'name': 'Arch1',\n",
    "        'num_conv_layers': 1,\n",
    "        'num_filters': [32],\n",
    "        'kernel_sizes': [3],\n",
    "        'fc_units': 64,\n",
    "        'dropout_rate': 0.2,\n",
    "    },\n",
    "    {\n",
    "        'name': 'Arch2',\n",
    "        'num_conv_layers': 2,\n",
    "        'num_filters': [32, 64],\n",
    "        'kernel_sizes': [3, 3],\n",
    "        'fc_units': 128,\n",
    "        'dropout_rate': 0.3,\n",
    "    },\n",
    "     {\n",
    "        'name': 'Arch3',\n",
    "        'num_conv_layers': 2,\n",
    "        'num_filters': [64, 128],\n",
    "        'kernel_sizes': [5, 5],\n",
    "        'fc_units': 256,\n",
    "        'dropout_rate': 0.5,\n",
    "    },\n",
    "      {\n",
    "        'name': 'Arch4',\n",
    "        'num_conv_layers': 2,\n",
    "        'num_filters': [128, 256],\n",
    "        'kernel_sizes': [5, 5],\n",
    "        'fc_units': 512,\n",
    "        'dropout_rate': 0.5,\n",
    "    },\n",
    "       {\n",
    "        'name': 'Arch5',\n",
    "        'num_conv_layers': 2,\n",
    "        'num_filters': [64, 64],\n",
    "        'kernel_sizes': [3, 3],\n",
    "        'fc_units': 128,\n",
    "        'dropout_rate': 0.3,\n",
    "    },\n",
    "    {\n",
    "        'name': 'Arch6',\n",
    "        'num_conv_layers': 3,\n",
    "        'num_filters': [32, 64, 128],\n",
    "        'kernel_sizes': [3, 3, 3],\n",
    "        'fc_units': 128,\n",
    "        'dropout_rate': 0.3,\n",
    "    },\n",
    "   \n",
    "    {\n",
    "        'name': 'Arch7',\n",
    "        'num_conv_layers': 3,\n",
    "        'num_filters': [64, 128, 256],\n",
    "        'kernel_sizes': [3, 3, 3],\n",
    "        'fc_units': 256,\n",
    "        'dropout_rate': 0.5,\n",
    "    },\n",
    "   {\n",
    "        'name': 'Arch8',\n",
    "        'num_conv_layers': 3,\n",
    "        'num_filters': [32, 64, 64],\n",
    "        'kernel_sizes': [5, 5, 5],\n",
    "        'fc_units': 128,\n",
    "        'dropout_rate': 0.4,\n",
    "    },\n",
    "   \n",
    "    {\n",
    "        'name': 'Arch9',\n",
    "        'num_conv_layers': 3,\n",
    "        'num_filters': [64, 128, 256],\n",
    "        'kernel_sizes': [5, 3, 3],\n",
    "        'fc_units': 256,\n",
    "        'dropout_rate': 0.3,\n",
    "    },\n",
    "    {\n",
    "        'name': 'Arch10',\n",
    "        'num_conv_layers': 4,\n",
    "        'num_filters': [32, 64, 128, 256],\n",
    "        'kernel_sizes': [5, 3, 3, 3],\n",
    "        'fc_units': 128,\n",
    "        'dropout_rate': 0.5,\n",
    "    },\n",
    "     {\n",
    "        'name': 'Arch11',\n",
    "        'num_conv_layers': 4,\n",
    "        'num_filters': [32, 64, 128, 256],\n",
    "        'kernel_sizes': [3, 3, 3, 3],\n",
    "        'fc_units': 128,\n",
    "        'dropout_rate': 0.4,\n",
    "    },\n",
    "   \n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(arch):\n",
    "    model = models.Sequential(name=arch['name'])\n",
    "    input_shape = (IMAGE_SIZE[0], IMAGE_SIZE[1], 3)\n",
    "    model.add(layers.InputLayer(shape=input_shape))  # Updated line\n",
    "    for i in range(arch['num_conv_layers']):\n",
    "        filters = arch['num_filters'][i]\n",
    "        kernel_size = arch['kernel_sizes'][i]\n",
    "        model.add(layers.Conv2D(filters, kernel_size, padding='same', activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(arch['fc_units'], activation='relu'))\n",
    "    model.add(layers.Dropout(arch['dropout_rate']))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Architecture Evaluation...\n",
      "\n",
      "Evaluating Architecture: Arch1\n",
      "Found 750 validated image filenames belonging to 2 classes.\n",
      "Found 188 validated image filenames belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johan/anaconda3/lib/python3.9/site-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johan/anaconda3/lib/python3.9/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "2024-11-20 16:12:31.540979: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:2: Filling up shuffle buffer (this may take a while): 3 of 8\n",
      "2024-11-20 16:12:41.554865: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:2: Filling up shuffle buffer (this may take a while): 5 of 8\n",
      "/home/johan/anaconda3/lib/python3.9/site-packages/PIL/Image.py:3368: DecompressionBombWarning: Image size (96631920 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "2024-11-20 16:12:59.105016: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 6s/step - accuracy: 0.6261 - loss: 20.7845 - val_accuracy: 0.7713 - val_loss: 0.5665\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 16:15:37.541424: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:2: Filling up shuffle buffer (this may take a while): 3 of 8\n",
      "2024-11-20 16:15:53.278604: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:2: Filling up shuffle buffer (this may take a while): 6 of 8\n",
      "2024-11-20 16:16:03.608045: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 6s/step - accuracy: 0.6846 - loss: 1.5306 - val_accuracy: 0.5160 - val_loss: 0.6883\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 16:18:25.140126: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:2: Filling up shuffle buffer (this may take a while): 2 of 8\n",
      "2024-11-20 16:18:37.110531: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:2: Filling up shuffle buffer (this may take a while): 4 of 8\n",
      "2024-11-20 16:18:49.466037: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:2: Filling up shuffle buffer (this may take a while): 6 of 8\n",
      "2024-11-20 16:19:04.902539: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 6s/step - accuracy: 0.6507 - loss: 1.0013 - val_accuracy: 0.6277 - val_loss: 0.6494\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 16:21:24.910401: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:2: Filling up shuffle buffer (this may take a while): 2 of 8\n",
      "2024-11-20 16:21:36.109292: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:2: Filling up shuffle buffer (this may take a while): 4 of 8\n",
      "2024-11-20 16:21:55.180584: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 5s/step - accuracy: 0.6652 - loss: 0.7444 - val_accuracy: 0.6809 - val_loss: 0.6097\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 6s/step\n",
      "\n",
      "Metrics for Architecture Arch1:\n",
      "Accuracy: 0.7713\n",
      "TP: 90, FP: 39, TN: 55, FN: 4\n",
      "Sensitivity (Recall): 0.9574\n",
      "Specificity: 0.5851\n",
      "Precision: 0.6977\n",
      "F1 Score: 0.8072\n",
      "Area Under ROC Curve: 0.9056\n",
      "\n",
      "Evaluating Architecture: Arch2\n",
      "Found 750 validated image filenames belonging to 2 classes.\n",
      "Found 188 validated image filenames belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johan/anaconda3/lib/python3.9/site-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johan/anaconda3/lib/python3.9/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "2024-11-20 16:25:10.274162: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 3 of 8\n",
      "2024-11-20 16:25:22.841603: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 5 of 8\n",
      "2024-11-20 16:25:39.353301: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     43\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[1;32m     46\u001b[0m val_generator\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    322\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:919\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m   \u001b[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[1;32m    914\u001b[0m   filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    915\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(\n\u001b[1;32m    916\u001b[0m           bound_args\n\u001b[1;32m    917\u001b[0m       )\n\u001b[1;32m    918\u001b[0m   )\n\u001b[0;32m--> 919\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    920\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn_with_cond\u001b[39m(inner_args, inner_kwds):\n\u001b[1;32m    925\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1567\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluate architectures\n",
    "results = []\n",
    "learning_rate = 0.001\n",
    "\n",
    "print(\"\\nStarting Architecture Evaluation...\")\n",
    "for arch in architectures:\n",
    "    print(f\"\\nEvaluating Architecture: {arch['name']}\")\n",
    "    model = create_model(arch)\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Create generators for this architecture\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        x_col='filename',\n",
    "        y_col='class',\n",
    "        target_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "    \n",
    "    val_generator = val_datagen.flow_from_dataframe(\n",
    "        dataframe=val_df,\n",
    "        x_col='filename',\n",
    "        y_col='class',\n",
    "        target_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='binary',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Implement early stopping\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    # Training\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_generator,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluation\n",
    "    val_generator.reset()\n",
    "    preds = model.predict(val_generator)\n",
    "    y_pred = (preds > 0.5).astype(int).flatten()\n",
    "    y_true = val_generator.classes\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0  # Sensitivity\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    roc_auc = roc_auc_score(y_true, preds) if not np.isnan(preds).any() else 0.0\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nMetrics for Architecture {arch['name']}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")\n",
    "    print(f\"Sensitivity (Recall): {recall:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Area Under ROC Curve: {roc_auc:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'architecture': arch['name'],\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'tn': tn,\n",
    "        'fn': fn,\n",
    "        'sensitivity': recall,\n",
    "        'specificity': specificity,\n",
    "        'precision': precision,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nArchitectures Evaluation Results:\")\n",
    "print(results_df[['architecture', 'accuracy', 'precision', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']])\n",
    "\n",
    "# Find the best architecture based on your preferred metric (e.g., accuracy)\n",
    "best_architecture_name = results_df.sort_values(by='accuracy', ascending=False).iloc[0]['architecture']\n",
    "best_model_info = results_df[results_df['architecture'] == best_architecture_name].iloc[0]\n",
    "best_model = best_model_info['model']\n",
    "best_architecture = next(arch for arch in architectures if arch['name'] == best_architecture_name)\n",
    "\n",
    "print(f\"\\nBest Architecture: {best_architecture_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K-Fold Cross Validation on Best Architecture\n",
    "print(f\"\\nConducting K-Fold Validation on Best Architecture: {best_architecture_name}\")\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=seed)\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(image_paths, labels)):\n",
    "    print(f\"\\nFold {fold+1}/{NUM_FOLDS}\")\n",
    "    train_paths_fold, val_paths_fold = image_paths[train_idx], image_paths[val_idx]\n",
    "    train_labels_fold, val_labels_fold = labels[train_idx], labels[val_idx]\n",
    "    \n",
    "    # Create DataFrames for generators with string labels\n",
    "    train_df_fold = pd.DataFrame({\n",
    "        'filename': train_paths_fold,\n",
    "        'class': [rev_class_labels[label] for label in train_labels_fold]\n",
    "    })\n",
    "\n",
    "    val_df_fold = pd.DataFrame({\n",
    "        'filename': val_paths_fold,\n",
    "        'class': [rev_class_labels[label] for label in val_labels_fold]\n",
    "    })\n",
    "    \n",
    "    # Create generators\n",
    "    train_generator_fold = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_df_fold,\n",
    "        x_col='filename',\n",
    "        y_col='class',\n",
    "        target_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "    \n",
    "    val_generator_fold = val_datagen.flow_from_dataframe(\n",
    "        dataframe=val_df_fold,\n",
    "        x_col='filename',\n",
    "        y_col='class',\n",
    "        target_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='binary',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(best_architecture)\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Implement early stopping\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    # Training\n",
    "    history = model.fit(\n",
    "        train_generator_fold,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_generator_fold,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluation\n",
    "    val_generator_fold.reset()\n",
    "    preds = model.predict(val_generator_fold)\n",
    "    y_pred = (preds > 0.5).astype(int).flatten()\n",
    "    y_true = val_generator_fold.classes\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0  # Sensitivity\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    roc_auc = roc_auc_score(y_true, preds) if not np.isnan(preds).any() else 0.0\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nMetrics for Fold {fold+1}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")\n",
    "    print(f\"Sensitivity (Recall): {recall:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Area Under ROC Curve: {roc_auc:.4f}\")\n",
    "    \n",
    "    # Store metrics\n",
    "    fold_metrics.append({\n",
    "        'fold': fold + 1,\n",
    "        'accuracy': accuracy,\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'tn': tn,\n",
    "        'fn': fn,\n",
    "        'sensitivity': recall,\n",
    "        'specificity': specificity,\n",
    "        'precision': precision,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc\n",
    "    })\n",
    "\n",
    "# Average metrics over folds\n",
    "kfold_results_df = pd.DataFrame(fold_metrics)\n",
    "avg_metrics = kfold_results_df.mean()\n",
    "print(\"\\nK-Fold Cross-Validation Results:\")\n",
    "print(kfold_results_df[['fold', 'accuracy', 'precision', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']])\n",
    "print(\"\\nAverage Metrics over all folds:\")\n",
    "print(avg_metrics[['accuracy', 'precision', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning on the best architecture\n",
    "print(\"\\nPerforming Hyperparameter Tuning on the Best Architecture\")\n",
    "\n",
    "# Define hyperparameters for tuning\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "dropout_rates = [0.3, 0.5]\n",
    "batch_sizes = [16, 32, 64]\n",
    "\n",
    "tuning_results = []\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "for lr, dr, bs in product(learning_rates, dropout_rates, batch_sizes):\n",
    "    print(f\"\\nEvaluating Hyperparameters: LR={lr}, Dropout={dr}, Batch Size={bs}\")\n",
    "    arch = best_architecture.copy()\n",
    "    arch['dropout_rate'] = dr\n",
    "    \n",
    "    # Adjust batch size and create generators\n",
    "    train_generator_tune = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        x_col='filename',\n",
    "        y_col='class',\n",
    "        target_size=IMAGE_SIZE,\n",
    "        batch_size=bs,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "\n",
    "    val_generator_tune = val_datagen.flow_from_dataframe(\n",
    "        dataframe=val_df,\n",
    "        x_col='filename',\n",
    "        y_col='class',\n",
    "        target_size=IMAGE_SIZE,\n",
    "        batch_size=bs,\n",
    "        class_mode='binary',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(arch)\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=lr),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Implement early stopping\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    # Training\n",
    "    history = model.fit(\n",
    "        train_generator_tune,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_generator_tune,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluation\n",
    "    val_generator_tune.reset()\n",
    "    preds = model.predict(val_generator_tune)\n",
    "    y_pred = (preds > 0.5).astype(int).flatten()\n",
    "    y_true = val_generator_tune.classes\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0  # Sensitivity\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    roc_auc = roc_auc_score(y_true, preds) if not np.isnan(preds).any() else 0.0\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nMetrics for Hyperparameters LR={lr}, Dropout={dr}, Batch Size={bs}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")\n",
    "    print(f\"Sensitivity (Recall): {recall:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Area Under ROC Curve: {roc_auc:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    tuning_results.append({\n",
    "        'learning_rate': lr,\n",
    "        'dropout_rate': dr,\n",
    "        'batch_size': bs,\n",
    "        'accuracy': accuracy,\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'tn': tn,\n",
    "        'fn': fn,\n",
    "        'sensitivity': recall,\n",
    "        'specificity': specificity,\n",
    "        'precision': precision,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc\n",
    "    })\n",
    "\n",
    "# Create a DataFrame with the tuning results\n",
    "tuning_results_df = pd.DataFrame(tuning_results)\n",
    "print(\"\\nHyperparameter Tuning Results:\")\n",
    "print(tuning_results_df[['learning_rate', 'dropout_rate', 'batch_size', 'accuracy', 'precision', 'sensitivity', 'specificity', 'f1_score', 'roc_auc']])\n",
    "\n",
    "# Find the best hyperparameters based on your preferred metric (e.g., highest specificity with high accuracy)\n",
    "best_tuning_index = tuning_results_df.sort_values(by=['specificity', 'accuracy'], ascending=False).index[0]\n",
    "best_hyperparams = tuning_results_df.loc[best_tuning_index]\n",
    "print(f\"\\nBest Hyperparameters:\")\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Training with Best Hyperparameters on Full Dataset\n",
    "print(\"\\nTraining Final Model with Best Hyperparameters on Full Dataset\")\n",
    "\n",
    "# Update architecture and hyperparameters\n",
    "final_architecture = best_architecture.copy()\n",
    "final_architecture['dropout_rate'] = best_hyperparams['dropout_rate']\n",
    "final_learning_rate = best_hyperparams['learning_rate']\n",
    "final_batch_size = best_hyperparams['batch_size']\n",
    "\n",
    "# Create DataFrame with string labels\n",
    "full_df = pd.DataFrame({\n",
    "    'filename': image_paths,\n",
    "    'class': [rev_class_labels[label] for label in labels]\n",
    "})\n",
    "\n",
    "# Create generator on full dataset\n",
    "train_generator_full = train_datagen.flow_from_dataframe(\n",
    "    dataframe=full_df,\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=final_batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Create model\n",
    "final_model = create_model(final_architecture)\n",
    "final_model.compile(optimizer=optimizers.Adam(learning_rate=final_learning_rate),\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# Implement early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Training\n",
    "history = final_model.fit(\n",
    "    train_generator_full,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the final model\n",
    "final_model.save('final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Final Model on Training Data\n",
    "train_generator_full.reset()\n",
    "preds = final_model.predict(train_generator_full)\n",
    "y_pred = (preds > 0.5).astype(int).flatten()\n",
    "y_true = train_generator_full.classes\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0  # Sensitivity\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "roc_auc = roc_auc_score(y_true, preds) if not np.isnan(preds).any() else 0.0\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nFinal Model Evaluation Metrics on Full Dataset:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")\n",
    "print(f\"Sensitivity (Recall): {recall:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Area Under ROC Curve: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training Accuracy and Loss over Time\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.title('Training Accuracy Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion Matrix\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Fire', 'Fire'], yticklabels=['No Fire', 'Fire'])\n",
    "plt.title('Confusion Matrix on Full Dataset')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
